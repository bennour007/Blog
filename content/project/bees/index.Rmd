---
title: Bees
date: 2022-01-15T12:07:59+01:00
draft: true
editor_options: 
  chunk_output_type: console
---

### Intro

This post is an attempt to explore the bee colonies dataset from the Tidy Tuesday challenge. During this post we will be asking questions as we explore and understand the data, and hopefully answer them in a the form of a chart at the end. 

### setup the workspace


Let's import the libraries we might need, beware, I always start with the `tidyverse` first, and add other libraries as I go. 

```{r}
library(tidyverse)
library(gt)
library(ggcorrplot)
library(Hmisc)
```

Now let's import the data 

```{r}
colony <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-11/colony.csv')


```


### Explore the data

Of course a good start is to read the data dictionary provided by the authors very carefully. How does the data look like though? I will use the `gt` package to output the table in a pretty format for the blog, but this is not necessary.

```{r}
colony %>% 
  top_n(n = 10) %>% 
  gt() %>% 
  tab_header(
    title = 'Quarterly colony statistics from 2015 to 2021 in each US state'
  )
```

#### What the information looks like visually?

Visualizing data is very important, i would like to see if there's anything weird that I need to check or take it into consideration in further steps. My first reflex is to see how the number of colonies change over time in each state. In this case I should combine both year and and months columns just to have a unique date for each observation for the sake of simplicity. we can do that using the `unite` function.

```{r}
data_ends <- colony %>% 
  unite(
    'date',
    year:months,
    sep = ' '
  ) %>% 
  filter(
    date == '2018 October-December' 
  )

colony %>% 
  unite(
    'date',
    year:months,
    sep = ' '
  ) %>% 
  ggplot(
    aes(
      x = date,
      y = colony_n
    )
  ) +
  geom_line(
    aes(
      group = state
    )
  ) +
  ggrepel::geom_text_repel(
    aes(
      label = state
    ), 
    data = data_ends
  ) +
  theme(axis.text.x = element_text(angle = 45))
```


So, from here we can observe 3 things: 

  - There is no data points for April-July Quarter (Q2) of 2019
  - There are 3 main over-amplified data points state wise, those are California, North Dakota, and Florida. Also Note the presence of a National average, that could have been misleading.
  - A clear seasonality, probably due to the nature of bee activities and the average weather in that state.
  
In the plot above I used `geom_text_repel` from the `ggrepel` package to annotate each state just where the data point are missing for the sake of clarity. In order to do that I created the same table filtered by the date at that point, i.e: '2018 October-December'.   

Note that I'm not making any thematic changes for this plot, I'm only using this as a simple way to see what I need to keep in mind, or fix in further steps, so presentation isn't really important here. 

**Data Tweaking:**

  - First we need to get unique dates so we don't have to do it over and over.
  - Second, we must eliminate the national average from the colony table. It would be useful to look at it individually  later on, but its always better to summarize the data on my own. 
  - Third, I need to investigate the missing values, as if they are missing, or just have the value of 0. 

#### uniting the date columns and removing the national Average


```{r}
colony <- colony %>% 
   unite(
    'date',
    year:months,
    sep = ' '
  ) %>%   
  filter(state != 'United States' & state != 'Other States')

data_ends_filtered <- colony %>% 
  filter(
    date == '2018 October-December' 
  )
```


Great, now we can focus on the colony table.

```{r}
colony %>% 
  ggplot(
    aes(
      x = date,
      y = colony_n
    )
  ) +
  geom_line(
    aes(
      group = state
    )
  ) +
  ggrepel::geom_text_repel(
    aes(
      label = state
    ), 
    data = data_ends_filtered
  )+
  theme(axis.text.x = element_text(angle = 45))
```

I wonder why there's so much fluctuation in the colony numbers in some states? We can investigate this further by simply checking deviation from each state during the whole period, then we will have a better metric to compare them with.

```{r}
colony %>% 
  group_by(state) %>% 
  summarise(
    deviation = sd(colony_n, na.rm = T) %>% 
      round(., 0)
  ) %>% 
  arrange(desc(deviation)) %>% 
  mutate(
    state = fct_reorder(state, deviation)
  ) %>% 
  ggplot(
    aes(
      x = deviation,
      y = state
    )
  )+
  geom_col() + 
  ggrepel::geom_text_repel(
    aes(
      label = deviation
    )
  )
```

```{r}
colony %>% 
  group_by(date) %>% 
  summarise(
    colony_n = mean(colony_n, na.rm = T),
    colony_lost = mean(colony_lost, na.rm = T),
    colony_added = mean(colony_added, na.rm = T),
    colony_reno = mean(colony_reno, na.rm = T),
  ) %>% 
  ungroup() %>% 
  pivot_longer(
    cols=c(colony_n, colony_lost, colony_added, colony_reno), 
    names_to = "colony_state",
    values_to = "number"
  ) %>% 
  mutate(
    colony_state = as_factor(colony_state),
    colony_state = fct_relevel(colony_state, 
                               'colony_n', 'colony_lost', 'colony_added', 'colony_reno' )  
  ) %>%
  ggplot() + 
  geom_col(
    aes(
      date,
      number,
      fill = colony_state
    ),
    position = position_fill()
  ) +
  theme(axis.text.x = element_text(angle = 90))
```


All these numbers on their own, do not give us a summerising idea of what we need to learn.

What we want to see is, how are colonies doing from one year to the other given the renovation to loss ratio.. 

so if we 

let's try something different ey.

```{r}

df_states <-
  read_csv("content/project/bees/50_us_states_all_data.csv", col_names = F) %>% 
  select(state = "X2", ISO2 = "X3")%>%
  add_row(state = "District of Colombia", ISO2 = "DC")
```

```{r}
map_hex <- geojson_read("content/project/bees/us_states_hexgrid.geojson.json",  what = "sp")
map_hex@data <-
  map_hex@data %>%
  mutate(google_name = gsub(" \\(United States\\)", "", google_name))
## fortify
map_hex_fortified <- tidy(map_hex, region = "google_name")
## centroids for labels
centroids <- cbind.data.frame(data.frame(gCentroid(map_hex, byid = T),
                                         id = map_hex@data$iso3166_2,
                                         id_long = str_wrap(map_hex@data$google_name, 12)))

